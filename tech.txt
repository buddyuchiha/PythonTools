Техническое задание (ТЗ) для мини-проекта на Python: Асинхронный веб-краулер
Цель проекта
Разработать асинхронный веб-краулер, который одновременно обрабатывает несколько веб-страниц, извлекает их заголовки и статусы, а затем сохраняет данные в JSON-файл.

Функциональные требования
1. Асинхронные HTTP-запросы
Краулер должен:

Принимать список URL-адресов (минимум 5, включая один нерабочий для теста).

Для каждого URL выполнять GET-запрос асинхронно, используя aiohttp.

Извлекать:

HTTP-статус (200, 404, 500 и др.).

Заголовок страницы (тег <title>). Если заголовка нет, возвращать "No title".

2. Ограничение скорости запросов
Чтобы избежать блокировки IP или перегрузки сервера, нужно:

Ограничить скорость запросов до 2 запроса в секунду.

Использовать asyncio.sleep() или семафоры (asyncio.Semaphore).

3. Обработка ошибок
Краулер должен корректно обрабатывать:

Таймауты (если сайт не отвечает 5+ секунд).

HTTP-ошибки (404, 500 и др.).

Проблемы с подключением (например, несуществующий домен).

В случае ошибки записывать в результат статус "Error".

4. Сохранение результатов
Собранные данные сохранять в JSON-файл в таком формате:

json
[
    {
        "url": "https://example.com",
        "status": 200,
        "title": "Example Domain"
    },
    {
        "url": "https://broken-url.com",
        "status": "Error",
        "title": null
    }
]
5. Логирование
Выводить в консоль информацию о ходе выполнения:

Какой URL обрабатывается.

Успешные запросы ([OK] 200: https://example.com).

Ошибки ([ERROR] Timeout: https://unreachable-site.com).